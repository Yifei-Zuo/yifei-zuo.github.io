<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Quantum-Inspired Linear Attention with Complex Embeddings | Yifei Ethan Zuo</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/posts">Post</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Quantum-Inspired Linear Attention with Complex Embeddings</span></h1>

<h2 class="date">2023/09/18</h2>
</div>

<main>
<p>Recent advancements in long-context language models emphasize the importance of positional encoding, allowing the Transformer architecture to grasp long-range dependencies. The seminal paper &ldquo;<em>Attention is All You Need</em>&rdquo; introduced positional encoding as an external piece of information since the original Transformer architecture is permutation invariant. Furthermore, to encode distance relatively, the Rotary Positional Embedding (RoPE) was proposed. We will see that RoPE fits more intuitively within the framework of linear recurrence as a result of internal assumptions instead of external inductive-bias. And we will derive complex linear Attention based on this framework.</p>
<blockquote>
<p>Some notational conventions:</p>
<ul>
<li>Capital letters are used for matrices. They&rsquo;re real-valued unless otherwise specified.</li>
<li>We use lowercase letters for vectors and use Dirac notation if they are complex-valued. The inner product of Hilbert space is denoted as <code>$\langle \cdot \rvert \cdot \rangle$</code>.</li>
<li>For Transformers, we use <code>$ q , k, v$</code> to denote the query, key and value projections of input <code>$ x$</code> respectively. The hidden dimension is <code>$d$</code> by default.</li>
</ul>
</blockquote>
<p>State space method is a powerful tool for analyzing dynamic systems. Albert Gu introduced SSM (State Space Model) into time series and language modeling. A typical SSM is defined as follows:</p>
<p><code>$$ \begin{aligned} \frac{d s}{dt} &amp;= A s + B u \\ y &amp;= C s + D u \end{aligned} $$</code></p>
<p>Where <code>$s$</code> is the state representation with system input <code>$u$</code> and output <code>$y$</code>. Linear attention is a state space model with a specific input and output format. Here we only consider linear attention with linear kernel.</p>
<p><code>$$ \begin{aligned} \frac{d S}{dt} &amp;= A s + k v^T \\ \mathsf{linearAttn}(x_t) &amp;= q_t^T S \end{aligned} $$</code></p>
<p>The solution to the above linear differential equation with zero initial condition is a convolution.</p>
<p><code>$$ \begin{aligned} s &amp;= \int_0^t e^{A(t-\tau)} k v^T d\tau \\ \mathsf{linearAttn}(x) &amp;= \int_0^t q^T_te^{A(t-\tau)} k v^T d\tau \\ &amp;= \int_0^t q^T_tUe^{\Lambda(t-\tau)}U^H k v^T d\tau \\ \xrightarrow[\text{}]{\text{discrete}} &amp;= \sum_{m=1}^n q^T_nU\Lambda^{n-m}U^H k_m v_m^T \end{aligned} $$</code></p>
<p>Calculating the power series <code>$e^{A(t-\tau)}$</code> is computationally intensive. A natural remedy involves diagonalization. Assume that <code>$A$</code> can be diagonalized in complex field. Here we decompose <code>$A$</code> as <code>$ A= U \Lambda U^H$</code>, where <code>$U$</code> is unitary and <code>$ \Lambda$</code> is diagonal matrix of eigenvalues. The diagonalizability, which requires the the sum of the geometric multiplicities of all its eigenvalues equals to <code>$d$</code>, is a mild assumption considering the fact that the set of all matrices with distinct eigenvalues is dense in the set of all matrices. Specifically, we have the following lemma.</p>
<p><strong>Lemma (Theorem 1 in Hartfiel (1995)).</strong> <em>Real matrices with <code>$k$</code> distinct nonzero eigenvalues are dense in the set of all <code>$d\times d$</code> real matrices with rank at most <code>$k$</code>, where <code>$0&lt;k\le d$</code>.</em></p>
<p>If we further assume that all eigenvalues are exclusively complex (absence of real eigenvalues), and owing to the properties of eigenvalue decomposition, these complex eigenvalues, paired with their respective eigenvectors, come in conjugate pairs. The following orthogonal basis transformation converts the matrix <code>$U, \Lambda$</code> into its real-valued counterpart. For each 2-d subspace, the transformation is as follows:</p>
<p><code>$$ \begin{aligned} \begin{bmatrix} \mathbf v_i &amp; \mathbf v_i^* \end{bmatrix}\begin{bmatrix} \lambda_i &amp; 0 \\ 0 &amp; \lambda_i^* \end{bmatrix}=\begin{bmatrix} \mathfrak R\{\mathbf v_i\} &amp; \mathfrak I\{\mathbf v_i\} \end{bmatrix}\begin{bmatrix} \alpha_i\begin{pmatrix} cos\phi_i &amp; -sin\phi_i\\ sin\phi_i &amp; cos\phi_i \end{pmatrix} \end{bmatrix} \end{aligned} $$</code></p>
<p>This transform facilitates the merging of the unitary matrix with the query and key projection matrix <code>$W_q, W_k$</code>. This process offers a comparable interpretation to RoPE, especially when the eigenvectors of <code>$A$</code> are normalized to be unitary i.e. <code>$\alpha_i=1, i=1,2,\dots,\frac{d}{2}$</code>. If scaled by a consistently decaying factor <code>$\gamma$</code>, it parallels the retentive mechanism. We denote the rotation matrix <code>$\mathcal{R}_{C}$</code> as a rotation operator in Hilbert space and we use <code>$\mathcal R$</code> to denote the transformed rotation matrix by convention, which is used as an element-wise operator. The eventual converted linear attention mechanism is as follows.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_t) &amp;= \int_0^t q^T_t\mathcal R^{t-\tau} k v^T d\tau \xrightarrow[\text{}]{\text{discrete}} \sum_{m=1}^n q^T_n\mathcal R^{n-m} k_mv_m^T \end{aligned} $$</code></p>
<p>For clarity, we just use the complex formulation and merge the <code>$U, U^H$</code> matrices into the query and key projection <code>$W_q, W_k$</code>, resulting in query and key embeddings in complex form. For consistency, we also convert the value embedding into complex field as well.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_n) &amp;= \sum_{m=1}^n \langle q_n\rvert \mathcal R_C^{n-m} \lvert k_m\rangle \lvert v_m\rangle,\text{ where }\lvert q_n\rangle, \lvert k_m\rangle, \lvert v_m\rangle \in \mathbb C^{\frac{d}{2}} \end{aligned} $$</code></p>
<p>In the standard attention methodology, we employ the softmax function on the real part of query-key inner product which convert the real value series into a probability distribution. Specifically, softmax functions result in a Boltzmann distribution which maximizes the system&rsquo;s entropy, under the constraint that the expected microstate energy remains constant. To be specific, the element in attention weight is as follows:</p>
<p><code>$$ \alpha_j = \frac{\exp(\mathfrak{R}(\langle q_n\rvert \mathcal R_C^{n-j} \lvert k_j\rangle))}{\sum_{m=1}^n \exp(\mathfrak{R}(\langle q_n\rvert \mathcal R_C^{n-m} \lvert k_m\rangle))} $$</code></p>
<p>It is somewhat counterintuitive that the standard Attention mechanism exclusively employs the real part of the complex inner product<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. What implications arise if we also incorporate the imaginary part? Can it offer any added advantages? Note that there isn&rsquo;t a definitive linear operation that converts a series of real values directly into a probability distribution and therefore non-linearity is inevitable.</p>
<p>We hypothesize that the exponential distribution class introduces a specific inductive bias, which may not be optimal for all tasks, especially in long context scenarios where standard attention mechanisms are prone to forgetting in the middle of the sequence. Fortunately, by retaining the imaginary part of the complex inner product, we can leverage a well-defined probabilistic distribution, which is grounded in the deep physical insights provided by quantum mechanics, where probabilities are determined by projecting the state vector onto different subspaces and computing the squared length of this projection.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_n) &amp;=\sum_{m=1}^n \lvert v_m\rangle \langle q_n\rvert \mathcal R_C^{m-n} \lvert k_m\rangle \\ &amp;= \sum_{m=1}^n C_{n,m} e^{\phi j}\lvert v_m\rangle \\ &amp;= \sum_{m=1}^n \frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}}\rvert^2} [\underbrace{(\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}) e^{\phi j}}_{\text{can be merged into }W_v}\lvert v_m\rangle] \end{aligned} $$</code></p>
<p>Given that <code>$\sum_{m=1}^n(\frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}})^2=1$</code> and <code>$C_{n,m}\in \mathbb R$</code> is non-negative, the coefficient transforms into a probability distribution for a random variable in Hilbert space. If the state itself is normalized i.e. <code>$\langle \psi \rvert \psi\rangle=1$</code>, the normalization of probability automatically holds.</p>
<p>In practical implementations, instead of directly determining the coefficient&rsquo;s normalization factor or normalizing the state, one could opt for applying group normalization to each head. This approach holds the potential for the model to self-learn the normalization process for the coefficient, therefore we just use the unnormalized states.
From here, we can decouple the complex embedding into its real counterpart by placing the real and imaginary components in separate dimensions. This adjustment allows for seamless integration with the multi-head attention mechanism.</p>
<h2 id="sample-complexity-of-distribution-approximation">Sample Complexity of Distribution Approximation</h2>
<h2 id="position-interpolation">Position Interpolation</h2>
<p>We consider the position interpolation in our model where the limit of time step <code>$\Delta t \to 0$</code>. We can just consider the continue formulation of the linear attention mechanism where the matrix power should be replaced by matrix exponential <code>$e^{tA}$</code>. We consider the transition matrix <code>$A$</code> a skew-symmetric matrix as a special case. The eigenvalue of a skew-symmetric matrix is purely imaginary or zero and</p>
<h2 id="backward-pass">Backward Pass</h2>
<p>The backward pass is operated in complex field as a result, which requires us to properly calculate the gradient of complex matrix. Empirically, we can just take the real part and imaginary part as separate matrices and calculate their gradients respectively. In our experiment this approach does optimize the network, but it is not theoretically correct in terms of gradient calculation of complex functions.</p>
<p>Moreover, consider function <code>$f: \mathbb C \mapsto \mathbb C$</code> s.t. <code>$f(z)=u(x,y)+jv(x,y)$</code>, where <code>$z=x+jy$</code> and <code>$u,v$</code> are real-valued functions. In order for the complex derivative of function <code>$f$</code> to exist in standard holomorphic sense, the real partial derivates of <code>$u$</code> and <code>$v$</code> must exist and satisfy the <em>Cauchy-Riemann condition</em>, which is a strong condition and is not satisfied by most functions. In order to extend the complex derivative to functions that are not holomorphic, we typically use <em>Wirtinger derivative</em> which can be reduced to the standard partial derivative when the function is holomorphic. We here consider functions over conjugate coordinates</p>
<p><code>$$ c := (z, \bar z)^T \in \mathbb C \times \mathbb C $$</code>
The Wirtinger derivative of function <code>$f$</code> can be formally defined as the following pair of partial derivatives:</p>
<p><code>$$ \begin{align} &amp;\frac{\partial f(z, \bar z)}{\partial z}\rvert_{\bar z=\text{const}} \\ &amp;\frac{\partial f(z, \bar z)}{\partial \bar z}\rvert_{z=\text{const}} \end{align} $$</code></p>
<p>This can be extended to multi-variable case naturally. The cogradient operator and its conjugate over high-dimensional complex coordinates are defined as follows:</p>
<p><code>$$ \begin{aligned} \frac{\partial}{\partial \mathbf z} &amp;:= (\frac{\partial}{\partial z_1} \dots \frac{\partial}{\partial z_n}) = \frac{\partial}{\partial \mathbf x} - j\frac{\partial}{\partial \mathbf y} \\ \frac{\partial}{\partial \overline{\mathbf z}} &amp;:= (\frac{\partial}{\partial \bar z_1} \dots \frac{\partial}{\partial \bar z_n}) = \frac{\partial}{\partial \mathbf x} + j\frac{\partial}{\partial \mathbf y} \end{aligned} $$</code></p>
<p>When we do optimization on real-valued loss, the step we should take while making variable update is given by the conjugate of the cogradient.</p>
<p><code>$$ \nabla f = f - \eta \frac{\partial f}{\partial \overline{\mathbf z}} $$</code></p>
<p>The detailed backward calculation is as follows:</p>
<p><code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \vecr{\pl{\bar V}} &amp;= \vecr{\pl{O}\color{blue}{\pd{O}{\bar V}} + \pl{\bar O}\pd{\bar O}{\bar V}} \\ &amp;= (\mathbb I \otimes P^H) \vecr{\pl{\bar O}} \\ &amp;= \vecr{P^H\pl{\bar O}}\\ \vecr{\pl{\bar P}} &amp;= \vecr{\pl{O}\color{blue}{\pd{O}{\bar P}} + \pl{\bar O}\pd{\bar O}{\bar P}} \\ &amp;= (\bar V\otimes \mathbb I) \vecr{\pl{\bar O}} \\ &amp;= \vecr{\pl{\bar O} V^H} \end{aligned} $$</code></p>
<p>Therefore we get
<code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \pl{\bar V} &amp;= P^H\pl{\bar O} \\ \pl{\bar P} &amp;= \pl{\bar O} V^H \\ \end{aligned} $$</code></p>
<p>Similarly, we calculate the gradient of matrix <code>$Q, K$</code>:</p>
<p><code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \vecr{\pl{\bar Q}} &amp;= \vecr{\pl{P}\color{blue}{\pd{P}{\bar Q}} + \pl{\bar P}\pd{\bar P}{\bar Q}} \\ &amp;= (K^T \otimes \mathbb I) \vecr{\pl{\bar P}} \\ &amp;= \vecr{\pl{\bar P} K} \\ \vecr{\pl{\bar K}} &amp;= \vecr{\pl{P}\pd{P}{\bar K} + \pl{\bar P}\color{blue}{\pd{\bar P}{\bar K}}} \\ &amp;= (\mathbb I\otimes Q) \vecr{\pl{P}} \\ &amp;= \vecr{Q \pl{P}} \end{aligned} $$</code></p>
<p>Therefore we get
<code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \pl{\bar Q} &amp;= \pl{\bar P} K \\ \pl{\bar K} &amp;= Q \pl{P} \\ \end{aligned} $$</code></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>It should be clarified that while retentive networks express their formulas in the complex domain, their actual implementation only involves real part. Refer to their official implementation <a href="https://github.com/microsoft/torchscale">https://github.com/microsoft/torchscale</a> for more details.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</main>

  <footer>
  <script defer src="//yihui.org/js/math-code.js"></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script defer src="//yihui.org/js/center-img.js"></script>
  
  <hr/>
  Â© <a href="https://dune-z.github.io">Yifei Ethan Zuo</a> 2022 &ndash; 2023
  
  </footer>
  </body>
</html>


<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Yifei Ethan Zuo</title>
    <link>/</link>
    <description>Recent content in Home on Yifei Ethan Zuo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Sep 2023 14:32:37 -0700</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quantum-Inspired Linear Attention with Complex Embeddings</title>
      <link>/posts/quantum-inspired-complex-embeddings-in-linear-transformers/</link>
      <pubDate>Mon, 18 Sep 2023 14:32:37 -0700</pubDate>
      
      <guid>/posts/quantum-inspired-complex-embeddings-in-linear-transformers/</guid>
      <description>Recent advancements in long-context language models emphasize the importance of positional encoding, allowing the Transformer architecture to grasp long-range dependencies. The seminal paper &amp;ldquo;Attention is All You Need&amp;rdquo; introduced positional encoding as an external piece of information since the original Transformer architecture is permutation invariant. Furthermore, to encode distance relatively, the Rotary Positional Embedding (RoPE) was proposed. The main flaw of current Transformer in long context scenarios is the quadratic complexity of attention matrix calculation and the memory usage of KV cache dominates the memory usage of the model.</description>
    </item>
    
  </channel>
</rss>

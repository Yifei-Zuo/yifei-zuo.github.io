<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Yifei Ethan Zuo</title>
    <link>/</link>
    <description>Recent content in Home on Yifei Ethan Zuo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Sep 2023 15:39:31 -0700</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>USTC Learning Theory Seminar</title>
      <link>/posts/ustc-learning-theory-seminar/</link>
      <pubDate>Fri, 29 Sep 2023 15:39:31 -0700</pubDate>
      
      <guid>/posts/ustc-learning-theory-seminar/</guid>
      <description>This page serves as links to the notes on learning theory seminar at USTC, launched by several graduate and undergraduate students.
[Sample](/posts/files/High-dimensional\ probability\ an\ introduction.pdf) </description>
    </item>
    
    <item>
      <title>Quantum-Inspired Complex Embeddings in Linear Transformers</title>
      <link>/posts/complex-linear-transformer/</link>
      <pubDate>Mon, 18 Sep 2023 14:32:37 -0700</pubDate>
      
      <guid>/posts/complex-linear-transformer/</guid>
      <description>Recent advancements in long-context language models emphasize the importance of positional encoding, allowing the Transformer architecture to grasp long-range dependencies. The seminal paper &amp;ldquo;Attention is All You Need&amp;rdquo; introduced positional encoding as an external piece of information since the original Transformer architecture is permutation invariant. Furthermore, to encode distance relatively, the Rotary Positional Embedding (RoPE) was proposed. We will see that RoPE fits more intuitively within the framework of linear recurrence as a result of internal assumptions instead of external inductive-bias.</description>
    </item>
    
  </channel>
</rss>

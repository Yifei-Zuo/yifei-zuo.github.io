<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Complex Linear Transformer | Yifei Ethan Zuo</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/posts">Post</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Complex Linear Transformer</span></h1>

<h2 class="date">2023/09/18</h2>
</div>

<main>
<p>Recent advancements in long-context language models emphasize the importance of positional encoding, allowing the Transformer architecture to grasp long-range dependencies. The seminal paper &ldquo;Attention is All You Need&rdquo; introduced positional encoding as an external piece of information since the original Transformer architecture is permutation invariant. Furthermore, to encode distance relatively, the Rotary Positional Embedding (RoPE) was proposed. We will see that RoPE fits more intuitively within the framework of linear recurrence.</p>
<blockquote>
<p>Some notational conventions:</p>
<ul>
<li>Capital letters are used for matrices. They&rsquo;re real-valued unless otherwise specified.</li>
<li>We use lowercase letters for vectors and use Dirac notation if they are complex-valued.</li>
<li>For Transformers, we use <code>$ q , k, v$</code> to denote the query, key and value projections of input <code>$ x$</code> respectively. The hidden dimension is <code>$d$</code> by default.</li>
</ul>
</blockquote>
<p>Consider element-wise linear recurrence with input <code>$ k_n v_n^T$</code> and recurrent state <code>$ S_n\in\mathbb R^{d\times d}$</code>:
<code>$$ \begin{aligned} S_n &amp;=  A S_{n-1} +  k_n v_n^T = \sum_{m=1}^n A^{n-m} k_m v_m^T \end{aligned} $$</code>
Here we assume <code>$ S_0=0$</code>. Calculating the matrix power <code>$ A^n$</code> is computationally intensive. A natural remedy involves diagonalization. While only symmetric matrices are diagonalizable in the real field, we explore the complex scenario, making mild assumptions on <code>$A$</code>.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> If we decompose <code>$A$</code> into its eigenvalues as <code>$ A= U \Lambda U^H$</code>, where <code>$U$</code> is unitary and <code>$ \Lambda$</code> is diagonal, the Linear Transformer processes the recurrent hidden state $ S_n$ by multiplying it with the query projection <code>$q_n^T$</code>, yielding the attention output.</p>
<p><code>$$ \begin{aligned} S_n &amp;= \sum_{m=1}^n U \Lambda^{n-m} U^H k_m v_m^T \\ \mathsf{attention}(\mathbb x_n) &amp;= \sum_{m=1}^n  q_n^T U \Lambda^{n-m} U^H k_m v_m^T \\ \end{aligned} $$</code></p>
<p>Further assuming all eigenvalues of <code>$A$</code> are exclusively complex (absence of real eigenvalues), and owing to the properties of eigenvalue decomposition, these complex eigenvalues, paired with their respective eigenvectors, come in conjugate pairs. An orthogonal basis transformation<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, which converts the matrix <code>$U, \Lambda$</code> into its real-valued counterpart, facilitates the merging of the unitary matrix with the query and key projection. This process offers a comparable interpretation to RoPE, especially when the eigenvectors of <code>$A$</code> are normalized to be unitary. If scaled by a consistently decaying factor <code>$\gamma$</code>, it parallels the retentive mechanism. We denote the rotation matrix as <code>$\mathcal R$</code> by convention.</p>
<p>It is somewhat counterintuitive that the standard Attention mechanism exclusively employs the real part of the complex inner product<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. What implications arise if we also incorporate the imaginary part? Can it offer any added advantages? In the standard attention methodology, we employ the softmax function on the real value series, transforming the series into a probability distribution. Specifically, this results in a Boltzmann distribution which maximizes the system&rsquo;s entropy, under the constraint that the expected microstate energy remains constant. Note that there isn&rsquo;t a definitive linear operation that converts a series of real values directly into a probability distribution.</p>
<p>However, by retaining the imaginary part of the complex inner product, we can leverage a well-defined probabilistic distribution. This distribution is grounded in the deep physical insights provided by quantum mechanics. It should be evident as we normalize the amplitude of inner product in the following derivation.</p>
<p><code>$$ \begin{aligned} \lvert o_n\rangle &amp;=\sum_{m=1}^n \lvert v_m\rangle \langle q_n\rvert R^{m-n} \lvert k_m\rangle \\ &amp;= \sum_{m=1}^n C_{n,m} e^{\phi j}\lvert v_m\rangle \\ &amp;= \sum_{m=1}^n \frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}}\rvert^2} [\underbrace{(\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}) e^{\phi j}}_{\text{can be merged into }W_v}\lvert v_m\rangle] \end{aligned} $$</code></p>
<p>Given that <code>$\sum_{m=1}^n(\frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}})^2=1$</code> and $C_{n,m}\in \mathbb R$ is non-negative, the coefficient transforms into a probability distribution for a random variable within the Hilbert space. From here, we can decouple the complex embedding into its real counterpart by placing the real and imaginary components in separate dimensions. This adjustment allows for seamless integration with the multi-head attention mechanism. In practical implementations, instead of directly determining the coefficient&rsquo;s normalization factor, one could opt for applying group normalization to each head. This approach holds the potential for the model to self-learn the normalization process for the coefficient.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Matrices in <code>$R^{d\times d}$</code> are diagonalizable in complex field if and only if the sum of the geometric multiplicities of all its eigenvalues equals to <code>$d$</code>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>For each 2-d subspace, just notice that<code>$\begin{aligned} \begin{bmatrix} v &amp;  v^* \end{bmatrix} \begin{bmatrix} \lambda &amp; 0\\ 0 &amp; \lambda^* \end{bmatrix}&amp;=\begin{bmatrix} Re\{ v\} &amp; Im\{ v\} \end{bmatrix}\begin{bmatrix} Re\{\lambda\} &amp; -Im\{\lambda\}\\ Im\{\lambda\} &amp; Re\{\lambda\} \end{bmatrix} \end{aligned}$</code>. Here <code>$v$</code> is the eigenvector corresponding to eigenvalue <code>$\lambda$</code>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>It should be clarified that while retentive networks express their formulas in the complex domain, their actual implementation only involves real part. Refer to their official implementation <a href="https://github.com/microsoft/torchscale">https://github.com/microsoft/torchscale</a> for more details.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>We just need to make the training process stable .e.g. avoid gradient explosion/vanishing and numerical instability. In our experiments, no more normalization is needed besides the group normalization.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</main>

  <footer>
  <script defer src="//yihui.org/js/math-code.js"></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script defer src="//yihui.org/js/center-img.js"></script>
  
  <hr/>
  Â© <a href="https://dune-z.github.io">Yifei Ethan Zuo</a> 2022 &ndash; 2023
  
  </footer>
  </body>
</html>

